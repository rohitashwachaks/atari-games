{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea57533",
   "metadata": {
    "id": "7ea57533"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AKIIMBXohYos",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKIIMBXohYos",
    "outputId": "22867184-1cfb-4fb1-8f2f-b326ce7c477e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !python -m atari_py.import_roms /content/drive/MyDrive/Optimization_II/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb9808",
   "metadata": {
    "id": "3feb9808"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape, action_space, lr):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten(input_shape=input_shape)(X_input)\n",
    "    X = Dense(512, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    action = Dense(action_space, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "    value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    Actor = Model(inputs = X_input, outputs = action)\n",
    "    Actor.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=lr))\n",
    "\n",
    "    Critic = Model(inputs = X_input, outputs = value)\n",
    "    Critic.compile(loss='mse', optimizer=RMSprop(learning_rate=lr))\n",
    "\n",
    "    return Actor, Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "actor, critic = create_model((4,80,80),env.action_space.n,0.000025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5512243",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0187a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42464cc",
   "metadata": {
    "id": "a42464cc"
   },
   "outputs": [],
   "source": [
    "class Pong_Algorithm:\n",
    "    # Actor-Critic Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.train_games = 5000\n",
    "        self.test_games = 200\n",
    "        self.max_average = -21.0\n",
    "        self.lr = 0.000025\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "\n",
    "        # Instantiate games and plot memory\n",
    "        self.frames, self.actions, self.rewards = [], [], []\n",
    "        self.scores, self.games, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        self.frame_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.frame_buffer = np.zeros(self.frame_size)\n",
    "        \n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = f'{self.env_name}_A2C_{self.lr}'\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.Actor, self.Critic = create_model(input_shape=self.frame_size, action_space = self.action_size, lr=self.lr)\n",
    "#         self.Actor = load_model('NN_WinPong_AC2_Actor_1268.tf')\n",
    "#         self.Critic = load_model('NN_WinPong_AC2_Critic_1268.tf')\n",
    "\n",
    "    def store_feed(self, feed, action, reward):\n",
    "        # store game actions to memory\n",
    "        self.frames.append(feed)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def decide_action(self, feed):\n",
    "        prediction = self.Actor.predict(feed)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        return action\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the delta-discounted rewards over an game\n",
    "        delt = 0.99 # discount factor\n",
    "        nr = r.shape[0]\n",
    "        discounted_r = np.zeros(nr)\n",
    "        for t in range(nr):\n",
    "            # start at the end\n",
    "            if r[nr-t-1] > 0:\n",
    "                discounted_r[nr-t-1] = 1 \n",
    "            elif r[nr-t-1] < 0: \n",
    "                discounted_r[nr-t-1] = -1\n",
    "            elif t==0: \n",
    "                discounted_r[nr-t-1] = 0\n",
    "            elif discounted_r[nr-t-1] == 0: \n",
    "                discounted_r[nr-t-1] = delt*discounted_r[nr-t]\n",
    "        return discounted_r\n",
    "                \n",
    "    def play_a_game(self):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        feed = np.vstack(self.frames)\n",
    "        actions = np.vstack(self.actions)\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(self.rewards)\n",
    "        # Get Critic network predictions\n",
    "        predictions = self.Critic.predict(feed)[:, 0]\n",
    "        # Compute weights (advantages)\n",
    "        weights = discounted_r - predictions\n",
    "\n",
    "        # training Actor and Critic networks\n",
    "        self.Actor.fit(feed, actions, sample_weight=weights, epochs=1, verbose=0)\n",
    "        self.Critic.fit(feed, discounted_r, epochs=1, verbose=0)\n",
    "        # save models\n",
    "        self.Actor.save('NN_WinPong_AC2_Actor_temp.tf')\n",
    "        self.Critic.save('NN_WinPong_AC2_Critic_temp.tf')\n",
    "        # reset training memory\n",
    "        self.frames, self.actions, self.rewards = [], [], []\n",
    "    \n",
    "    def load_actor(self, Actor_name):\n",
    "        self.Actor = load_model(Actor_name)\n",
    "\n",
    "    def save_scores(self, score, game):\n",
    "        self.scores.append(score)\n",
    "        self.games.append(game)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        df_scores = pd.DataFrame({'scores':agent.scores,'games':agent.games,'averages':agent.average})\n",
    "        df_scores.to_csv('/content/drive/MyDrive/Optimization_II/Scores_temp.csv')\n",
    "        print('Scores Saved!')\n",
    "        return self.average[-1]\n",
    "\n",
    "    def prepro(self, frame):\n",
    "        # cropping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # converting to black and white for faster training\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255    \n",
    "\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "        self.frame_buffer = np.roll(self.frame_buffer, 1, axis = 0)\n",
    "        self.frame_buffer[0,:,:] = new_frame\n",
    "        \n",
    "        return np.expand_dims(self.frame_buffer, axis=0)\n",
    "\n",
    "    def reset_environment(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            frame = self.prepro(frame)\n",
    "        return frame\n",
    "\n",
    "    def take_step(self, action):\n",
    "        pix_new, reward, done, info = self.env.step(action)\n",
    "        pix = self.prepro(pix_new)\n",
    "        return pix, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        for e in range(self.train_games):\n",
    "            current_frame = self.reset_environment()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                # Actor picks an action\n",
    "                action = self.decide_action(current_frame)\n",
    "                # Retrieve new state, reward, and whether the game is done\n",
    "                future_frame, reward, done, _ = self.take_step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                self.store_feed(current_frame, action, reward)\n",
    "                # Update current state\n",
    "                current_frame = future_frame\n",
    "                score += reward\n",
    "                if done:\n",
    "                    average = self.save_scores(score, e)\n",
    "                    self.play_a_game()\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Actor_name):\n",
    "        self.load(Actor_name)\n",
    "        game_scores = []\n",
    "        for e in range(self.test_games):\n",
    "            feed = self.reset_environment()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action = np.argmax(self.Actor.predict(feed))\n",
    "                pix, reward, done, _ = self.take_step(action)\n",
    "                score += reward    \n",
    "                if done:\n",
    "                    game_scores.append(score)\n",
    "                    print(f'Game: {e}/{self.test_games}, Score: {score}')\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc44d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "edfc44d2",
    "outputId": "9fedefba-282a-4e28-eaf8-1b354223c774"
   },
   "outputs": [],
   "source": [
    "env_name = 'Pong-v0'\n",
    "ac_pong = Pong_Algorithm(env_name)\n",
    "ac_pong.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa27c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_pong.test('NN_WinPong_AC2_Actor.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da282a",
   "metadata": {
    "id": "12da282a"
   },
   "outputs": [],
   "source": [
    "df_train_scores = pd.read_csv('Scores_All.csv')\n",
    "df_train_scores.drop_duplicates(subset = 'Game', keep = 'last', inplace = True)\n",
    "df_train_scores['50-day Average Score'] = df_train_scores['Score'].rolling(50, 1).mean()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Scores vs Games Trained')\n",
    "plt.plot(df_train_scores['Game'],df_train_scores['Score'], 'b', label = 'Score')\n",
    "plt.plot(df_train_scores['Game'],df_train_scores['50-day Average Score'], 'r', label = '50-day Average')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783056d",
   "metadata": {
    "id": "6783056d"
   },
   "outputs": [],
   "source": [
    "df_test_scores = pd.read_csv('scores_test.csv')\n",
    "df_test_scores['score'].hist()\n",
    "plt.title('Histogram of Test Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e32dc",
   "metadata": {
    "id": "8e9e32dc"
   },
   "outputs": [],
   "source": [
    "df_test_scores.describe()['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fdd88",
   "metadata": {
    "id": "011fdd88"
   },
   "outputs": [],
   "source": [
    "len(df_test_scores[df_test_scores['score']>0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Actor Critic v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
